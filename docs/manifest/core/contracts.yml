# =====================================================================
# üç£ Sushi Kitchen ‚Äî CONTRACTS (v1.0)
# Purpose: Machine-readable technical contracts for all services
# - Docker images, versions, ports, volumes, environment variables
# - Dependencies, capabilities, and conflict resolution
# - Resource requirements and scaling parameters
# - Network isolation and security configurations
# =====================================================================

schema_version: "1.0"
last_updated: "2025-09-16"

# ========================================================================
# CAPABILITY DEFINITIONS
# Capabilities represent functional interfaces that services provide/require
# This enables automatic dependency resolution and compatibility checking
# ========================================================================

capabilities:
  # Storage Capabilities
  vector_storage:
    description: "Vector database for embeddings and similarity search"
    providers: ["futomaki.qdrant", "futomaki.weaviate", "futomaki.chroma", "futomaki.pinecone"]
    interface: "REST API with vector operations"
    
  graph_storage:
    description: "Graph database for relationship and network data"
    providers: ["futomaki.neo4j", "futomaki.memgraph", "futomaki.arangodb"]
    interface: "Cypher/GraphQL query interface"
    
  relational_storage:
    description: "SQL database for structured data"
    providers: ["futomaki.postgresql", "futomaki.mysql"]
    interface: "SQL query interface"
    
  object_storage:
    description: "S3-compatible object storage for files and artifacts"
    providers: ["futomaki.minio", "futomaki.supabase"]
    interface: "S3 API compatibility"
    
  cache_storage:
    description: "In-memory caching and session storage"
    providers: ["futomaki.redis", "futomaki.valkey"]
    interface: "Redis protocol"

  # LLM & Inference Capabilities
  local_llm_inference:
    description: "Local large language model inference"
    providers: ["hosomaki.ollama", "hosomaki.vllm", "hosomaki.tgi", "hosomaki.triton"]
    interface: "OpenAI-compatible API"
    
  cloud_llm_gateway:
    description: "Gateway to cloud LLM providers"
    providers: ["hosomaki.litellm"]
    interface: "Unified LLM API"
    
  embedding_generation:
    description: "Text to vector embedding conversion"
    providers: ["hosomaki.ollama", "uramaki.clip", "uramaki.blip2"]
    interface: "Embedding API"

  # Media Processing Capabilities
  speech_to_text:
    description: "Audio transcription and speech recognition"
    providers: ["temaki.whisper", "temaki.faster-whisper"]
    interface: "Audio file to text API"
    
  text_to_speech:
    description: "Text to speech synthesis"
    providers: ["temaki.coqui-tts", "temaki.espeak"]
    interface: "Text to audio file API"
    
  image_generation:
    description: "AI-powered image generation"
    providers: ["uramaki.comfyui", "uramaki.automatic1111", "uramaki.invokeai"]
    interface: "Prompt to image API"
    
  image_analysis:
    description: "Computer vision and image understanding"
    providers: ["uramaki.clip", "uramaki.blip2", "uramaki.yolo"]
    interface: "Image analysis API"

  # Infrastructure Capabilities
  workflow_orchestration:
    description: "Visual workflow and automation builder"
    providers: ["hosomaki.n8n", "hosomaki.temporal", "temaki.windmill"]
    interface: "Workflow execution API"
    
  web_server:
    description: "HTTP server and reverse proxy"
    providers: ["hosomaki.caddy", "hosomaki.nginx", "hosomaki.traefik"]
    interface: "HTTP/HTTPS serving"
    
  monitoring:
    description: "Metrics collection and monitoring"
    providers: ["inari.prometheus", "inari.grafana"]
    interface: "Prometheus metrics API"
    
  authentication:
    description: "Identity and access management"
    providers: ["gunkanmaki.authentik", "gunkanmaki.keycloak"]
    interface: "OIDC/SAML/OAuth2"
    
  secrets_management:
    description: "Secure storage and management of secrets"
    providers: ["gunkanmaki.infisical", "gunkanmaki.vaultwarden"]
    interface: "Secrets API"

# ========================================================================
# NETWORK PROFILES
# Define network isolation patterns for different privacy/security needs
# ========================================================================

network_profiles:
  open_research:
    description: "Single network for research and development"
    networks:
      sushi_net:
        driver: bridge
        ipam:
          config:
            - subnet: "172.20.0.0/16"
    isolation_level: "none"
    
  business_confidential:
    description: "Segmented networks for business use"
    networks:
      sushi_frontend:
        driver: bridge
        internal: false
      sushi_backend:
        driver: bridge
        internal: true
      sushi_data:
        driver: bridge
        internal: true
    isolation_level: "moderate"
    
  legal_privilege:
    description: "Maximum isolation for sensitive data"
    networks:
      sushi_public:
        driver: bridge
        internal: false
      sushi_processing:
        driver: bridge
        internal: true
      sushi_storage:
        driver: bridge
        internal: true
      sushi_audit:
        driver: bridge
        internal: true
    isolation_level: "maximum"

# ========================================================================
# SERVICE CONTRACTS
# Complete technical specifications for each service
# ========================================================================

services:

# ----------------------------------------------------------------------
# HOSOMAKI - Core & Inference Services
# ----------------------------------------------------------------------

  hosomaki.n8n:
    name: "n8n Workflow Automation"
    docker:
      image: "n8nio/n8n:latest"
      platform: "linux/amd64"
      profiles: ["hosomaki"]
    ports:
      - container: 5678
        host_range: "5678"
        protocol: "tcp"
        description: "Web interface and API"
    volumes:
      - name: "n8n_data"
        mount: "/home/node/.n8n"
        type: "named"
        backup_priority: "high"
    environment:
      GENERIC_TIMEZONE: "${TIMEZONE:-UTC}"
      DB_TYPE: "postgresdb"
      DB_POSTGRESDB_HOST: "postgresql"
      DB_POSTGRESDB_PORT: "5432"
      DB_POSTGRESDB_DATABASE: "${N8N_DB_NAME:-n8n}"
      DB_POSTGRESDB_USER: "${N8N_DB_USER:-n8n}"
      DB_POSTGRESDB_PASSWORD: "${N8N_DB_PASSWORD}"
      N8N_ENCRYPTION_KEY: "${N8N_ENCRYPTION_KEY}"
      WEBHOOK_URL: "https://${DOMAIN}/webhook/"
    provides:
      - workflow_orchestration
    requires:
      - relational_storage
    suggests:
      - web_server
      - secrets_management
    resource_requirements:
      cpu_cores: 1
      memory_mb: 512
      storage_gb: 5
    scaling:
      min_replicas: 1
      max_replicas: 3
      scale_metric: "cpu_usage"
    healthcheck:
      endpoint: "/healthz"
      interval: "30s"
      timeout: "10s"
      retries: 3
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_frontend", "sushi_backend"]
      legal_privilege: ["sushi_public", "sushi_processing"]

  hosomaki.ollama:
    name: "Ollama Local LLM Server"
    docker:
      image: "ollama/ollama:latest"
      platform: "linux/amd64"
      profiles: ["hosomaki"]
    ports:
      - container: 11434
        host_range: "11434"
        protocol: "tcp"
        description: "LLM inference API"
    volumes:
      - name: "ollama_data"
        mount: "/root/.ollama"
        type: "named"
        backup_priority: "medium"
    environment:
      OLLAMA_HOST: "0.0.0.0"
      OLLAMA_MODELS: "/root/.ollama/models"
      OLLAMA_KEEP_ALIVE: "24h"
      CUDA_VISIBLE_DEVICES: "${CUDA_VISIBLE_DEVICES:-all}"
    provides:
      - local_llm_inference
      - embedding_generation
    device_requests:
      - driver: "nvidia"
        count: "all"
        capabilities: ["gpu"]
    resource_requirements:
      cpu_cores: 2
      memory_mb: 4096
      storage_gb: 50
      gpu_memory_mb: 8192
    scaling:
      min_replicas: 1
      max_replicas: 2
      scale_metric: "gpu_memory"
    healthcheck:
      endpoint: "/api/tags"
      interval: "30s"
      timeout: "10s"
      retries: 3
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_backend"]
      legal_privilege: ["sushi_processing"]

  hosomaki.litellm:
    name: "LiteLLM Gateway"
    docker:
      image: "ghcr.io/berriai/litellm:main-latest"
      platform: "linux/amd64"
      profiles: ["hosomaki"]
    ports:
      - container: 4000
        host_range: "4000"
        protocol: "tcp"
        description: "LLM proxy API"
    volumes:
      - name: "litellm_config"
        mount: "/app/config"
        type: "bind"
        source: "./config/litellm"
    environment:
      LITELLM_MASTER_KEY: "${LITELLM_MASTER_KEY}"
      DATABASE_URL: "${LITELLM_DATABASE_URL}"
      STORE_MODEL_IN_DB: "True"
    provides:
      - cloud_llm_gateway
    suggests:
      - relational_storage
      - cache_storage
    resource_requirements:
      cpu_cores: 1
      memory_mb: 1024
      storage_gb: 2
    scaling:
      min_replicas: 1
      max_replicas: 5
      scale_metric: "request_rate"
    healthcheck:
      endpoint: "/health"
      interval: "30s"
      timeout: "10s"
      retries: 3
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_frontend", "sushi_backend"]
      legal_privilege: ["sushi_public", "sushi_processing"]

  hosomaki.anythingllm:
    name: "AnythingLLM RAG Interface"
    docker:
      image: "mintplexlabs/anythingllm:latest"
      platform: "linux/amd64"
      profiles: ["hosomaki"]
    ports:
      - container: 3001
        host_range: "3001"
        protocol: "tcp"
        description: "Web interface"
    volumes:
      - name: "anythingllm_storage"
        mount: "/app/server/storage"
        type: "named"
        backup_priority: "high"
      - name: "anythingllm_hotdir"
        mount: "/app/collector/hotdir"
        type: "named"
    environment:
      STORAGE_DIR: "/app/server/storage"
      JWT_SECRET: "${ANYTHINGLLM_JWT_SECRET}"
      LLM_PROVIDER: "ollama"
      OLLAMA_BASE_PATH: "http://ollama:11434"
      EMBEDDING_ENGINE: "ollama"
      VECTOR_DB: "qdrant"
      QDRANT_ENDPOINT: "http://qdrant:6333"
    provides:
      - rag_interface
    requires:
      - local_llm_inference
      - vector_storage
    suggests:
      - embedding_generation
    resource_requirements:
      cpu_cores: 1
      memory_mb: 2048
      storage_gb: 10
    scaling:
      min_replicas: 1
      max_replicas: 3
      scale_metric: "active_sessions"
    healthcheck:
      endpoint: "/api/system"
      interval: "30s"
      timeout: "10s"
      retries: 3
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_frontend", "sushi_backend"]
      legal_privilege: ["sushi_public", "sushi_processing"]

  hosomaki.caddy:
    name: "Caddy Web Server"
    docker:
      image: "caddy:2-alpine"
      platform: "linux/amd64"
      profiles: ["hosomaki"]
    ports:
      - container: 80
        host_range: "80"
        protocol: "tcp"
        description: "HTTP"
      - container: 443
        host_range: "443"
        protocol: "tcp"
        description: "HTTPS"
      - container: 2019
        host_range: "2019"
        protocol: "tcp"
        description: "Admin API"
    volumes:
      - name: "caddy_data"
        mount: "/data"
        type: "named"
        backup_priority: "medium"
      - name: "caddy_config"
        mount: "/config"
        type: "named"
      - name: "caddy_caddyfile"
        mount: "/etc/caddy/Caddyfile"
        type: "bind"
        source: "./config/caddy/Caddyfile"
    environment:
      DOMAIN: "${DOMAIN}"
      EMAIL: "${SSL_EMAIL}"
    provides:
      - web_server
    resource_requirements:
      cpu_cores: 0.5
      memory_mb: 256
      storage_gb: 1
    scaling:
      min_replicas: 1
      max_replicas: 3
      scale_metric: "connection_count"
    healthcheck:
      endpoint: "/health"
      interval: "30s"
      timeout: "5s"
      retries: 3
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_frontend"]
      legal_privilege: ["sushi_public"]

# ----------------------------------------------------------------------
# FUTOMAKI - Knowledge & Storage Services  
# ----------------------------------------------------------------------

  futomaki.qdrant:
    name: "Qdrant Vector Database"
    docker:
      image: "qdrant/qdrant:latest"
      platform: "linux/amd64"
      profiles: ["futomaki"]
    ports:
      - container: 6333
        host_range: "6333"
        protocol: "tcp"
        description: "REST API"
      - container: 6334
        host_range: "6334"
        protocol: "tcp"
        description: "gRPC API"
    volumes:
      - name: "qdrant_storage"
        mount: "/qdrant/storage"
        type: "named"
        backup_priority: "critical"
    environment:
      QDRANT__SERVICE__HTTP_PORT: "6333"
      QDRANT__SERVICE__GRPC_PORT: "6334"
      QDRANT__LOG_LEVEL: "INFO"
    provides:
      - vector_storage
    resource_requirements:
      cpu_cores: 2
      memory_mb: 4096
      storage_gb: 100
    scaling:
      min_replicas: 1
      max_replicas: 3
      scale_metric: "memory_usage"
    healthcheck:
      endpoint: "/health"
      interval: "30s"
      timeout: "10s"
      retries: 3
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_backend", "sushi_data"]
      legal_privilege: ["sushi_processing", "sushi_storage"]

  futomaki.postgresql:
    name: "PostgreSQL Database"
    docker:
      image: "postgres:15-alpine"
      platform: "linux/amd64"
      profiles: ["futomaki"]
    ports:
      - container: 5432
        host_range: "5432"
        protocol: "tcp"
        description: "PostgreSQL"
    volumes:
      - name: "postgresql_data"
        mount: "/var/lib/postgresql/data"
        type: "named"
        backup_priority: "critical"
    environment:
      POSTGRES_DB: "${POSTGRES_DB:-sushi}"
      POSTGRES_USER: "${POSTGRES_USER:-sushi}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      PGDATA: "/var/lib/postgresql/data/pgdata"
    provides:
      - relational_storage
    resource_requirements:
      cpu_cores: 1
      memory_mb: 2048
      storage_gb: 50
    scaling:
      min_replicas: 1
      max_replicas: 1
      scale_metric: "none"
    healthcheck:
      command: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-sushi}"]
      interval: "30s"
      timeout: "10s"
      retries: 3
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_data"]
      legal_privilege: ["sushi_storage"]

  futomaki.redis:
    name: "Redis Cache"
    docker:
      image: "redis:7-alpine"
      platform: "linux/amd64"
      profiles: ["futomaki"]
    ports:
      - container: 6379
        host_range: "6379"
        protocol: "tcp"
        description: "Redis"
    volumes:
      - name: "redis_data"
        mount: "/data"
        type: "named"
        backup_priority: "low"
    environment:
      REDIS_PASSWORD: "${REDIS_PASSWORD}"
    command: ["redis-server", "--requirepass", "${REDIS_PASSWORD}"]
    provides:
      - cache_storage
    resource_requirements:
      cpu_cores: 0.5
      memory_mb: 1024
      storage_gb: 5
    scaling:
      min_replicas: 1
      max_replicas: 2
      scale_metric: "memory_usage"
    healthcheck:
      command: ["CMD", "redis-cli", "ping"]
      interval: "30s"
      timeout: "3s"
      retries: 3
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_backend", "sushi_data"]
      legal_privilege: ["sushi_processing", "sushi_storage"]

  futomaki.minio:
    name: "MinIO Object Storage"
    docker:
      image: "minio/minio:latest"
      platform: "linux/amd64"
      profiles: ["futomaki"]
    ports:
      - container: 9000
        host_range: "9000"
        protocol: "tcp"
        description: "S3 API"
      - container: 9001
        host_range: "9001"
        protocol: "tcp"
        description: "Console"
    volumes:
      - name: "minio_data"
        mount: "/data"
        type: "named"
        backup_priority: "high"
    environment:
      MINIO_ROOT_USER: "${MINIO_ROOT_USER:-admin}"
      MINIO_ROOT_PASSWORD: "${MINIO_ROOT_PASSWORD}"
      MINIO_DOMAIN: "${DOMAIN}"
    command: ["server", "/data", "--console-address", ":9001"]
    provides:
      - object_storage
    resource_requirements:
      cpu_cores: 1
      memory_mb: 1024
      storage_gb: 200
    scaling:
      min_replicas: 1
      max_replicas: 3
      scale_metric: "storage_usage"
    healthcheck:
      command: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: "30s"
      timeout: "20s"
      retries: 3
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_backend", "sushi_data"]
      legal_privilege: ["sushi_storage"]

# ----------------------------------------------------------------------
# TEMAKI - Voice & Audio Services
# ----------------------------------------------------------------------

  temaki.whisper:
    name: "Whisper STT Service"
    docker:
      image: "onerahmet/openai-whisper-asr-webservice:latest"
      platform: "linux/amd64"
      profiles: ["temaki"]
    ports:
      - container: 9000
        host_range: "9010"
        protocol: "tcp"
        description: "Whisper API"
    environment:
      ASR_MODEL: "base"
      ASR_ENGINE: "openai_whisper"
    device_requests:
      - driver: "nvidia"
        count: "all"
        capabilities: ["gpu"]
    provides:
      - speech_to_text
    resource_requirements:
      cpu_cores: 2
      memory_mb: 4096
      storage_gb: 10
      gpu_memory_mb: 4096
    scaling:
      min_replicas: 1
      max_replicas: 2
      scale_metric: "gpu_utilization"
    healthcheck:
      endpoint: "/health"
      interval: "30s"
      timeout: "10s"
      retries: 3
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_backend"]
      legal_privilege: ["sushi_processing"]

  temaki.coqui-tts:
    name: "Coqui TTS Service"
    docker:
      image: "coqui/tts:latest"
      platform: "linux/amd64"
      profiles: ["temaki"]
    ports:
      - container: 5002
        host_range: "5002"
        protocol: "tcp"
        description: "TTS API"
    volumes:
      - name: "coqui_models"
        mount: "/root/.local/share/tts"
        type: "named"
        backup_priority: "medium"
    provides:
      - text_to_speech
    resource_requirements:
      cpu_cores: 2
      memory_mb: 2048
      storage_gb: 15
    scaling:
      min_replicas: 1
      max_replicas: 2
      scale_metric: "cpu_usage"
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_backend"]
      legal_privilege: ["sushi_processing"]

# ----------------------------------------------------------------------
# URAMAKI - Visual & Creative Services
# ----------------------------------------------------------------------

  uramaki.comfyui:
    name: "ComfyUI Image Generation"
    docker:
      image: "yanwk/comfyui-boot:latest"
      platform: "linux/amd64"
      profiles: ["uramaki"]
    ports:
      - container: 8188
        host_range: "8188"
        protocol: "tcp"
        description: "ComfyUI Interface"
    volumes:
      - name: "comfyui_data"
        mount: "/data"
        type: "named"
        backup_priority: "medium"
    environment:
      CLI_ARGS: "--listen --port 8188"
    device_requests:
      - driver: "nvidia"
        count: "all"
        capabilities: ["gpu"]
    provides:
      - image_generation
    resource_requirements:
      cpu_cores: 4
      memory_mb: 8192
      storage_gb: 100
      gpu_memory_mb: 12288
    scaling:
      min_replicas: 1
      max_replicas: 2
      scale_metric: "gpu_memory"
    healthcheck:
      endpoint: "/system_stats"
      interval: "60s"
      timeout: "10s"
      retries: 3
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_backend"]
      legal_privilege: ["sushi_processing"]

# ----------------------------------------------------------------------
# INARI - Observability Services
# ----------------------------------------------------------------------

  inari.prometheus:
    name: "Prometheus Metrics"
    docker:
      image: "prom/prometheus:latest"
      platform: "linux/amd64"
      profiles: ["inari"]
    ports:
      - container: 9090
        host_range: "9090"
        protocol: "tcp"
        description: "Prometheus UI"
    volumes:
      - name: "prometheus_data"
        mount: "/prometheus"
        type: "named"
        backup_priority: "medium"
      - name: "prometheus_config"
        mount: "/etc/prometheus/prometheus.yml"
        type: "bind"
        source: "./config/prometheus/prometheus.yml"
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--storage.tsdb.retention.time=200h"
      - "--web.enable-lifecycle"
    provides:
      - monitoring
    resource_requirements:
      cpu_cores: 1
      memory_mb: 2048
      storage_gb: 50
    scaling:
      min_replicas: 1
      max_replicas: 1
      scale_metric: "none"
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_backend"]
      legal_privilege: ["sushi_audit"]

  inari.grafana:
    name: "Grafana Dashboards"
    docker:
      image: "grafana/grafana:latest"
      platform: "linux/amd64"
      profiles: ["inari"]
    ports:
      - container: 3000
        host_range: "3000"
        protocol: "tcp"
        description: "Grafana UI"
    volumes:
      - name: "grafana_data"
        mount: "/var/lib/grafana"
        type: "named"
        backup_priority: "medium"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: "${GRAFANA_ADMIN_PASSWORD}"
      GF_USERS_ALLOW_SIGN_UP: "false"
    requires:
      - monitoring
    provides:
      - dashboard_visualization
    resource_requirements:
      cpu_cores: 1
      memory_mb: 1024
      storage_gb: 10
    scaling:
      min_replicas: 1
      max_replicas: 2
      scale_metric: "active_users"
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_frontend", "sushi_backend"]
      legal_privilege: ["sushi_public", "sushi_audit"]

# ----------------------------------------------------------------------
# GUNKANMAKI - Security Services
# ----------------------------------------------------------------------

  gunkanmaki.authentik:
    name: "Authentik Identity Provider"
    docker:
      image: "ghcr.io/goauthentik/authentik:latest"
      platform: "linux/amd64"
      profiles: ["gunkanmaki"]
    ports:
      - container: 9000
        host_range: "9020"
        protocol: "tcp"
        description: "Web interface"
      - container: 9443
        host_range: "9443"
        protocol: "tcp"
        description: "HTTPS interface"
    volumes:
      - name: "authentik_media"
        mount: "/media"
        type: "named"
        backup_priority: "high"
      - name: "authentik_templates"
        mount: "/templates"
        type: "named"
    environment:
      AUTHENTIK_SECRET_KEY: "${AUTHENTIK_SECRET_KEY}"
      AUTHENTIK_BOOTSTRAP_PASSWORD: "${AUTHENTIK_BOOTSTRAP_PASSWORD}"
      AUTHENTIK_BOOTSTRAP_TOKEN: "${AUTHENTIK_BOOTSTRAP_TOKEN}"
      AUTHENTIK_POSTGRESQL__HOST: "postgresql"
      AUTHENTIK_POSTGRESQL__NAME: "${AUTHENTIK_DB_NAME:-authentik}"
      AUTHENTIK_POSTGRESQL__USER: "${AUTHENTIK_DB_USER:-authentik}"
      AUTHENTIK_POSTGRESQL__PASSWORD: "${AUTHENTIK_DB_PASSWORD}"
      AUTHENTIK_REDIS__HOST: "redis"
      AUTHENTIK_REDIS__PASSWORD: "${REDIS_PASSWORD}"
    requires:
      - relational_storage
      - cache_storage
    provides:
      - authentication
    resource_requirements:
      cpu_cores: 1
      memory_mb: 2048
      storage_gb: 5
    scaling:
      min_replicas: 1
      max_replicas: 3
      scale_metric: "active_sessions"
    networks:
      open_research: ["sushi_net"]
      business_confidential: ["sushi_frontend", "sushi_backend"]
      legal_privilege: ["sushi_public", "sushi_processing"]

# ========================================================================
# DEPENDENCY RESOLUTION RULES
# Define how services should be automatically resolved and configured
# ========================================================================

dependency_resolution:
  auto_resolve_providers: true
  prefer_lightweight: true
  
  # Default providers for common capabilities
  default_providers:
    vector_storage: "futomaki.qdrant"
    relational_storage: "futomaki.postgresql"
    cache_storage: "futomaki.redis"
    object_storage: "futomaki.minio"
    local_llm_inference: "hosomaki.ollama"
    web_server: "hosomaki.caddy"
    monitoring: "inari.prometheus"
    authentication: "gunkanmaki.authentik"

  # Conflict resolution
  conflicts:
    - services: ["futomaki.postgresql", "futomaki.mysql"]
      reason: "Multiple SQL databases not recommended"
      resolution: "Choose one based on application requirements"
    - services: ["futomaki.redis", "futomaki.valkey"]
      reason: "Multiple cache stores unnecessary"
      resolution: "Use Redis for broader compatibility"

  # Resource scaling rules
  scaling_rules:
    gpu_services:
      rule: "Limit to available GPU memory"
      services: ["hosomaki.ollama", "hosomaki.vllm", "uramaki.comfyui", "temaki.whisper"]
    storage_services:
      rule: "Monitor disk usage closely"
      services: ["futomaki.qdrant", "futomaki.postgresql", "futomaki.minio"]

# ========================================================================
# ENVIRONMENT CONFIGURATION TEMPLATES
# Pre-configured environment variable templates for common scenarios
# ========================================================================

environment_templates:
  development:
    description: "Development environment with relaxed security"
    global_env:
      LOG_LEVEL: "DEBUG"
      DEBUG_MODE: "true"
      SSL_VERIFY: "false"
      DATA_RETENTION_DAYS: "7"
    
  production:
    description: "Production environment with full security"
    global_env:
      LOG_LEVEL: "INFO"
      DEBUG_MODE: "false"
      SSL_VERIFY: "true"
      DATA_RETENTION_DAYS: "365"
      BACKUP_ENABLED: "true"
      MONITORING_ENABLED: "true"
    
  compliance:
    description: "Compliance-ready with audit logging"
    global_env:
      LOG_LEVEL: "INFO"
      AUDIT_LOGGING: "true"
      ENCRYPTION_AT_REST: "true"
      DATA_RETENTION_DAYS: "2555"  # 7 years
      PII_DETECTION: "true"
      ANONYMIZATION: "true"