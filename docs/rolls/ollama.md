# üç£ Ollama Roll

## Overview
Local LLM runner that serves models over a simple API.

## Why our Chef chose it
- Offline-friendly
- Small models run on modest machines
- Pairs nicely with LiteLLM

## How it fits Sushi Kitchen
- Typical platter(s): Hosomaki
- Works well with: LiteLLM, n8n
- Resource notes: CPU/GPU and RAM heavy for larger models

## Quick start (conceptual)
- Enable the platter that includes this roll.
- Configure env vars and credentials as needed.
- Access the UI/API at the documented port.

## Learn more
- Official site: https://ollama.com
- Docs: https://github.com/ollama/ollama/blob/main/README.md
- GitHub: https://github.com/ollama/ollama
